{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0d0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# Open the .tgz file\n",
    "with tarfile.open('languageID.tgz', 'r:gz') as tar:\n",
    "    # Extract all files in the archive\n",
    "    tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf147f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the prior for english is:  0.3333333333333333\n",
      "the prior for japanese is:  0.3333333333333333\n",
      "the prior for spanish is:  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Define the vocabulary of characters\n",
    "vocabulary = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "              'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ']\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = 3\n",
    "\n",
    "# Define the smoothing parameter\n",
    "alpha = 1/2\n",
    "\n",
    "# Load the dataset\n",
    "dataset_dir = 'languageID'\n",
    "files = os.listdir(dataset_dir)\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for filename in files:\n",
    "    with open(os.path.join(dataset_dir, filename), 'r') as f:\n",
    "        text = f.read().lower()\n",
    "        \n",
    "        #use the number 0-9 file as training data \n",
    "        if(len(filename) == 6):\n",
    "            X_train.append(text)\n",
    "            y_train.append(filename[0])\n",
    "                \n",
    "        #use the number 10-19 file as training data \n",
    "        else:\n",
    "            X_test.append(text)\n",
    "            y_test.append(filename[0])\n",
    "        \n",
    "# Preprocess the data\n",
    "X_train_count = []\n",
    "X_test_count = []\n",
    "for doc in X_train:\n",
    "    counts = Counter(doc)\n",
    "    X_train_count.append([counts[char] for char in vocabulary])\n",
    "    \n",
    "for doc in X_test:\n",
    "    counts = Counter(doc)\n",
    "    X_test_count.append([counts[char] for char in vocabulary])\n",
    "\n",
    "# Convert y to integer labels\n",
    "label_map = {'e': 0, 'j': 1, 's': 2}\n",
    "y_train = [label_map[label] for label in y_train]\n",
    "y_test = [label_map[label] for label in y_test]\n",
    "\n",
    "# Estimate the prior probabilities\n",
    "prior_probs = np.zeros(num_classes)\n",
    "for c in range(num_classes):\n",
    "    prior_probs[c] = (np.sum([1 for label in y_train if label == c]) + alpha) / (len(y_train) + num_classes * alpha)\n",
    "\n",
    "print(\"the prior for english is: \", prior_probs[0])\n",
    "print(\"the prior for japanese is: \", prior_probs[1])\n",
    "print(\"the prior for spanish is: \", prior_probs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e13455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the conditional probability for english is: \n",
      " [0.06016851 0.01113497 0.02151    0.02197258 0.10536924 0.01893276\n",
      " 0.01747894 0.04721626 0.05541054 0.00142078 0.00373369 0.02897737\n",
      " 0.02051875 0.05792169 0.0644639  0.01675202 0.0005617  0.05382455\n",
      " 0.06618206 0.08012556 0.02666446 0.00928465 0.01549645 0.00115645\n",
      " 0.01384437 0.00062779 0.17924996]\n"
     ]
    }
   ],
   "source": [
    "# Estimate the conditional probabilities\n",
    "cond_probs = np.zeros((num_classes, len(vocabulary)))\n",
    "for c in range(num_classes):\n",
    "    X_c = [X_train_count[i] for i in range(len(X_train_count)) if y_train[i] == c]\n",
    "    total_counts = np.sum(X_c, axis=0)\n",
    "    denom = np.sum(total_counts) + len(vocabulary) * alpha\n",
    "    cond_probs[c, :] = (total_counts + alpha) / denom\n",
    "    \n",
    "print(\"the conditional probability for english is: \\n\", cond_probs[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d65963e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the conditional probability for japanese is: \n",
      " [1.31765610e-01 1.08669066e-02 5.48586603e-03 1.72263182e-02\n",
      " 6.02047591e-02 3.87854223e-03 1.40116706e-02 3.17621161e-02\n",
      " 9.70334393e-02 2.34110207e-03 5.74094133e-02 1.43261470e-03\n",
      " 3.97987351e-02 5.67105769e-02 9.11632132e-02 8.73545547e-04\n",
      " 1.04825466e-04 4.28037318e-02 4.21747790e-02 5.69901115e-02\n",
      " 7.06174220e-02 2.44592753e-04 1.97421294e-02 3.49418219e-05\n",
      " 1.41514379e-02 7.72214263e-03 1.23449457e-01]\n",
      "the conditional probability for spanish is: \n",
      " [1.04560451e-01 8.23286362e-03 3.75258241e-02 3.97459221e-02\n",
      " 1.13810860e-01 8.60287996e-03 7.18448398e-03 4.53270019e-03\n",
      " 4.98597021e-02 6.62945947e-03 2.77512257e-04 5.29431717e-02\n",
      " 2.58086399e-02 5.41765595e-02 7.24923684e-02 2.42669051e-02\n",
      " 7.67783910e-03 5.92951189e-02 6.57704049e-02 3.56140730e-02\n",
      " 3.37023219e-02 5.88942678e-03 9.25040856e-05 2.49761031e-03\n",
      " 7.86284728e-03 2.68261848e-03 1.68264932e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"the conditional probability for japanese is: \\n\", cond_probs[1, :])\n",
    "print(\"the conditional probability for spanish is: \\n\", cond_probs[2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b762222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the bag-of-words vector x for e10.txt is: \n",
      " [164, 32, 53, 57, 311, 55, 51, 140, 140, 3, 6, 85, 64, 139, 182, 53, 3, 141, 186, 225, 65, 31, 47, 4, 38, 2, 498]\n"
     ]
    }
   ],
   "source": [
    "x10 = []\n",
    "y10 = []\n",
    "\n",
    "with open(os.path.join(dataset_dir, 'e10.txt'), 'r') as f:\n",
    "    text = f.read().lower()\n",
    "    x10.append(text)\n",
    "    y10.append(filename[0])\n",
    "    \n",
    "x10_count = []\n",
    "\n",
    "for doc in x10:\n",
    "    counts = Counter(doc)\n",
    "    x10_count.append([counts[char] for char in vocabulary])\n",
    "\n",
    "y10 = [label_map[label] for label in y10]\n",
    "\n",
    "print(' the bag-of-words vector x for e10.txt is: \\n', x10_count[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91c34db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log(p(x | y = e)): -7841.865447060635\n",
      "log(p(x | y = j)): -8771.433079075032\n",
      "log(p(x | y = s)): -8467.282044010557\n"
     ]
    }
   ],
   "source": [
    "predict_proba = []\n",
    "\n",
    "log_probs = np.zeros(3)\n",
    "for c in range(num_classes):\n",
    "    log_probs[c] = np.sum([np.log(cond_probs[c, j]) * doc[j] for j in range(len(vocabulary))])\n",
    "predict_proba = log_probs\n",
    "    \n",
    "print('log(p(x | y = e)):', predict_proba[0])\n",
    "print('log(p(x | y = j)):', predict_proba[1])\n",
    "print('log(p(x | y = s)):', predict_proba[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c406456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log(p(y = e | x)): -7841.532113727302\n",
      "log(p(y = j | x)): -8771.099745741698\n",
      "log(p(y = s | x)): -8466.948710677223\n"
     ]
    }
   ],
   "source": [
    "posterior_proba = prior_probs + predict_proba\n",
    "    \n",
    "print('log(p(y = e | x)):', posterior_proba[0])\n",
    "print('log(p(y = j | x)):', posterior_proba[1])\n",
    "print('log(p(y = s | x)):', posterior_proba[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db7b66d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the predicted results:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "the true results:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# Classify new documents\n",
    "y_pred = []\n",
    "for doc in X_test_count:\n",
    "    log_probs = np.log(prior_probs)\n",
    "    for c in range(num_classes):\n",
    "        log_probs[c] += np.sum([np.log(cond_probs[c, j]) * doc[j] for j in range(len(vocabulary))])\n",
    "    y_pred.append(np.argmax(log_probs))\n",
    "\n",
    "print('the predicted results: ', y_pred)\n",
    "print('the true results: ', y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
