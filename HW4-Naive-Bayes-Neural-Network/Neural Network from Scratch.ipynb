{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9701a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z)) # to avoid numerical instability\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "def cross_entropy_loss(y, y_hat):\n",
    "    return -np.sum(y * np.log(y_hat))\n",
    "\n",
    "def forward(x, W1, W2, W3):\n",
    "    a1 = x\n",
    "    z1 = np.dot(W1, a1)\n",
    "    a2 = sigmoid(z1)\n",
    "    z2 = np.dot(W2, a2)\n",
    "    a3 = sigmoid(z2)\n",
    "    z3 = np.dot(W3, a3)\n",
    "    y_hat = softmax(z3)\n",
    "    return a1, z1, a2, z2, a3, z3, y_hat\n",
    "\n",
    "def backward(x, y, W1, W2, W3, a1, z1, a2, z2, a3, z3, y_hat, lr):\n",
    "    delta4 = y_hat - y\n",
    "    delta3 = np.dot(W3.T, delta4) * a3 * (1 - a3)\n",
    "    delta2 = np.dot(W2.T, delta3) * a2 * (1 - a2)\n",
    "    grad_W3 = np.outer(delta4, a3)\n",
    "    grad_W2 = np.outer(delta3, a2)\n",
    "    grad_W1 = np.outer(delta2, a1)\n",
    "    W3 -= lr * grad_W3\n",
    "    W2 -= lr * grad_W2\n",
    "    W1 -= lr * grad_W1\n",
    "    return W1, W2, W3\n",
    "\n",
    "def train(X_train, y_train, n_epochs, lr, d1, d2, k, batch_size):\n",
    "    n_train, d = X_train.shape\n",
    "    #W1 = np.zeros((d1, d))\n",
    "    #W2 = np.zeros((d2, d1))\n",
    "    #W3 = np.zeros((k, d2))\n",
    "\n",
    "    W1 = np.random.randn(d1, d)\n",
    "    W2 = np.random.randn(d2, d1)\n",
    "    W3 = np.random.randn(k, d2)\n",
    "\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        permutation = np.random.permutation(n_train)\n",
    "        X_train = X_train[permutation]\n",
    "        y_train = y_train[permutation]\n",
    "\n",
    "        # training\n",
    "        loss = 0\n",
    "        acc = 0\n",
    "        for i in range(0, n_train, batch_size):\n",
    "            X_batch = X_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "            batch_size_actual = y_batch.shape[0]\n",
    "            for j in range(batch_size_actual):\n",
    "                x = X_batch[j]\n",
    "                y = y_batch[j]\n",
    "                a1, z1, a2, z2, a3, z3, y_hat = forward(x, W1, W2, W3)\n",
    "                \n",
    "                if np.argmax(y_hat) == np.argmax(y):\n",
    "                    acc += 1\n",
    "                    \n",
    "                loss += cross_entropy_loss(y, y_hat)\n",
    "                W1, W2, W3 = backward(x, y, W1, W2, W3, a1, z1, a2, z2, a3, z3, y_hat, lr)\n",
    "        loss /= n_train\n",
    "        acc /= n_train\n",
    "        \n",
    "        train_loss.append(loss)\n",
    "        train_acc.append(acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, train loss: {train_loss[-1]:.3f}, train accuracy: {train_acc[-1]:.3f}\")\n",
    "        \n",
    "    return W1, W2, W3, train_acc, train_loss\n",
    "\n",
    "def test(X_test, y_test, W1, W2, W3):\n",
    "    n_test = X_test.shape[0]\n",
    "    test_acc = []\n",
    "    # testing\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    for i in range(n_test):\n",
    "        x = X_test[i]\n",
    "        y = y_test[i]\n",
    "        _, _, _, _, _, _, y_hat = forward(x, W1, W2, W3)\n",
    "        loss += cross_entropy_loss(y, y_hat)\n",
    "        if np.argmax(y_hat) == np.argmax(y):\n",
    "            acc += 1\n",
    "\n",
    "    acc /= n_test   \n",
    "    test_acc.append(acc)\n",
    "\n",
    "    print(f\"test error: {1-test_acc[-1]:.3f}\")\n",
    "    \n",
    "# Define the transformation to be applied to the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Convert the image to a tensor\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # Normalize the tensor with the mean and standard deviation of the dataset\n",
    "])\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset = datasets.MNIST('mnist_data/', train=True, download=True, transform=transform)\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = datasets.MNIST('mnist_data/', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False)\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for x, y in train_loader:\n",
    "    X_train.append(np.matrix.flatten(np.array(x)))\n",
    "    one_hot = np.zeros(10)\n",
    "    one_hot[np.array(y)[0]] = 1\n",
    "    y_train.append(one_hot)  \n",
    "    \n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for x, y in test_loader:\n",
    "    X_test.append(np.matrix.flatten(np.array(x)))\n",
    "    one_hot = np.zeros(10)\n",
    "    one_hot[np.array(y)[0]] = 1\n",
    "    y_test.append(one_hot)  \n",
    "    \n",
    "W1, W2, W3, acc_curve, err_curve = train(np.array(X_train), np.array(y_train), 100, 0.05, 300, 200, 10, 32)\n",
    "\n",
    "test(np.array(X_test), np.array(y_test), W1, W2, W3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
